{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR-10 on extreme NON-IID dataset.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN3ju/FVE/7+8Lfeb6bWwDS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "58150d0349f94f8e86d1e1013be6378c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9e1e60c52c054612babd18aaa7ca57d3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bafaea4c6ed04f90a5ca8677b9ad85f9",
              "IPY_MODEL_b5884797e13c451b9b300608771d87bd"
            ]
          }
        },
        "9e1e60c52c054612babd18aaa7ca57d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bafaea4c6ed04f90a5ca8677b9ad85f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0d1da8df8cd342f8ba17fbee058a286c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_31d3eaebab2e4cebbcc9ae30c4df8d6d"
          }
        },
        "b5884797e13c451b9b300608771d87bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_afcd1a22f1824fc49d0758d857dcff7e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [33:53&lt;00:00, 83860.45it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4f081f12d75041448dd84a61c991118a"
          }
        },
        "0d1da8df8cd342f8ba17fbee058a286c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "31d3eaebab2e4cebbcc9ae30c4df8d6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "afcd1a22f1824fc49d0758d857dcff7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4f081f12d75041448dd84a61c991118a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfA7fyPj05wT"
      },
      "source": [
        "# Federated Learning (Extreme Non-IID setting)\n",
        "\n",
        "\n",
        "Train a centralized model on a decentralized data. Dataset used: CIFAR-10. In PyTorch, CIFAR 10 is available to use with the help of the torchvision module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooHMSpD01Jr2"
      },
      "source": [
        "# To import all the relevant packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mKBE7NE4nig"
      },
      "source": [
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision import transforms \n",
        "from torchvision.transforms import Compose \n",
        "torch.backends.cudnn.benchmark=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKFrkz7k1TMp"
      },
      "source": [
        "# Setting the Hyper-parameters\n",
        "\n",
        "**classes_pc:** classes per client, further this is used to divide the balanced dataset to non-IID dataset by creating an unbalanced representation of classes among the clients. For e.g. if the classes_pc=1, then all the clients will have images from one class only, thus creating an extensive imbalance among the clients.\n",
        "\n",
        "**num_clients:** Number of clients among which images are to be distributed.\n",
        "\n",
        "**num_selected:** Number of randomly selected clients from num_clients during the start of each communication round. To be used in the training phase of the global model. Typically, num_selected is around 30â€“40% of the num_clients.\n",
        "\n",
        "**num_rounds:** Total number of communication rounds for the global model to train. In each communication round, training on individual clients takes place simultaneously.\n",
        "\n",
        "**epochs:** Number of local training rounds on each clientâ€™s device.\n",
        "\n",
        "**batch_size:** Loading of the data into the data loader in batches.\n",
        "\n",
        "**baseline_num:** Total number of baseline images to be saved on the global server for retraining of the clientâ€™s model before aggregation. This technique of retraining all the models on the global server deals with non-IID/real-world datasets.\n",
        "\n",
        "**retrain_epochs:** Total number of retraining rounds on the global server after receiving the model weights from all the clients that participated in the communication round.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KImSjIx-4tEW"
      },
      "source": [
        "classes_pc = 2\n",
        "num_clients = 20\n",
        "num_selected = 6\n",
        "num_rounds = 50\n",
        "epochs = 5\n",
        "batch_size = 32\n",
        "baseline_num = 100\n",
        "retrain_epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RvnU2KXh6PA"
      },
      "source": [
        "# Creating the distribution\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW8E9vKr4viM"
      },
      "source": [
        "#### get cifar dataset in x and y form\n",
        "\n",
        "#The get_cifar10 function downloads the CIFAR10 dataset and returns x_train, y_train for training, and x_test, y_test for test purposes.\n",
        "\n",
        "def get_cifar10():\n",
        "  '''Return CIFAR10 train/test data and labels as numpy arrays'''\n",
        "  \n",
        "  #downloads the dataset from torchvision\n",
        "  data_train = torchvision.datasets.CIFAR10('./data', train=True, download=True)\n",
        "  data_test = torchvision.datasets.CIFAR10('./data', train=False, download=True) \n",
        "  \n",
        "  #converts it into NumPy array.\n",
        "  x_train, y_train = data_train.data.transpose((0,3,1,2)), np.array(data_train.targets)\n",
        "  x_test, y_test = data_test.data.transpose((0,3,1,2)), np.array(data_test.targets)\n",
        "  \n",
        "  return x_train, y_train, x_test, y_test\n",
        "\n",
        "#Function to print the basic data stats\n",
        "def print_image_data_stats(data_train, labels_train, data_test, labels_test):\n",
        "  print(\"\\nData: \")\n",
        "  print(\" - Train Set: ({},{}), Range: [{:.3f}, {:.3f}], Labels: {},..,{}\".format(\n",
        "    data_train.shape, labels_train.shape, np.min(data_train), np.max(data_train),\n",
        "      np.min(labels_train), np.max(labels_train)))\n",
        "  print(\" - Test Set: ({},{}), Range: [{:.3f}, {:.3f}], Labels: {},..,{}\".format(\n",
        "    data_test.shape, labels_test.shape, np.min(data_train), np.max(data_train),\n",
        "      np.min(labels_test), np.max(labels_test)))\n",
        "  \n",
        "#--------------------------------------------------------------------------------------------------------------------------------------#\n",
        "#The clients_rand function creates a random distribution for the clients, such that every client has an arbitrary number of images. \n",
        "#It is one of the helper functions to be used.\n",
        "\n",
        "def clients_rand(train_len, nclients):\n",
        "  '''\n",
        "  train_len: size of the train data\n",
        "  nclients: number of clients\n",
        "  \n",
        "  Returns: to_ret\n",
        "  \n",
        "  This function creates a random distribution \n",
        "  for the clients, i.e. number of images each client \n",
        "  possess.\n",
        "  '''\n",
        "  client_tmp=[]\n",
        "  sum_=0\n",
        "  #### creating random values for each client ####\n",
        "  for i in range(nclients-1):\n",
        "    tmp=random.randint(10,100)\n",
        "    sum_+=tmp\n",
        "    client_tmp.append(tmp)\n",
        "\n",
        "  client_tmp= np.array(client_tmp)\n",
        "  #### using those random values as weights ####\n",
        "  clients_dist= ((client_tmp/sum_)*train_len).astype(int)\n",
        "  num  = train_len - clients_dist.sum()\n",
        "  to_ret = list(clients_dist)\n",
        "  to_ret.append(num)\n",
        "  return to_ret\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------------------#\n",
        "#The split_image_data function splits the given images into n_clients. \n",
        "#It returns a split which is further used to create the real-world dataset. \n",
        "# verbose: specifies verbosity mode(0 = silent, 1= progress bar, 2 = one line per epoch).\n",
        "\n",
        "def split_image_data(data, labels, n_clients=100, classes_per_client=10, shuffle=True, verbose=True):\n",
        "  '''\n",
        "  Splits (data, labels) among 'n_clients s.t. every client can holds 'classes_per_client' number of classes\n",
        "  Input:\n",
        "    data : [n_data x shape]\n",
        "    labels : [n_data (x 1)] from 0 to n_labels\n",
        "    n_clients : number of clients\n",
        "    classes_per_client : number of classes per client\n",
        "    shuffle : True/False => True for shuffling the dataset, False otherwise\n",
        "    verbose : True/False => True for printing some info, False otherwise\n",
        "  Output:\n",
        "    clients_split : client data into desired format\n",
        "  '''\n",
        "  #### constants #### \n",
        "  n_data = data.shape[0]\n",
        "  n_labels = np.max(labels) + 1\n",
        "\n",
        "\n",
        "  ### client distribution ####\n",
        "  data_per_client = clients_rand(len(data), n_clients)\n",
        "  data_per_client_per_class = [np.maximum(1,nd // classes_per_client) for nd in data_per_client]\n",
        "  \n",
        "  # sort for labels\n",
        "  data_idcs = [[] for i in range(n_labels)]\n",
        "  for j, label in enumerate(labels):\n",
        "    data_idcs[label] += [j]\n",
        "  if shuffle:\n",
        "    for idcs in data_idcs:\n",
        "      np.random.shuffle(idcs)\n",
        "    \n",
        "  # split data among clients\n",
        "  clients_split = []\n",
        "  c = 0\n",
        "  for i in range(n_clients):\n",
        "    client_idcs = []\n",
        "        \n",
        "    budget = data_per_client[i]\n",
        "    c = np.random.randint(n_labels)\n",
        "    while budget > 0:\n",
        "      take = min(data_per_client_per_class[i], len(data_idcs[c]), budget)\n",
        "      \n",
        "      client_idcs += data_idcs[c][:take]\n",
        "      data_idcs[c] = data_idcs[c][take:]\n",
        "      \n",
        "      budget -= take\n",
        "      c = (c + 1) % n_labels\n",
        "      \n",
        "    clients_split += [(data[client_idcs], labels[client_idcs])]\n",
        "\n",
        "  def print_split(clients_split): \n",
        "    print(\"Data split:\")\n",
        "    for i, client in enumerate(clients_split):\n",
        "      split = np.sum(client[1].reshape(1,-1)==np.arange(n_labels).reshape(-1,1), axis=1)\n",
        "      print(\" - Client {}: {}\".format(i,split))\n",
        "    print()\n",
        "      \n",
        "    if verbose:\n",
        "      print_split(clients_split)\n",
        "  \n",
        "  clients_split = np.array(clients_split)\n",
        "  \n",
        "  return clients_split\n",
        "\n",
        "#To shuffle the images of each client respectively\n",
        "def shuffle_list(data):\n",
        "  '''\n",
        "  This function returns the shuffled data\n",
        "  '''\n",
        "  for i in range(len(data)):\n",
        "    tmp_len= len(data[i][0])\n",
        "    index = [i for i in range(tmp_len)]\n",
        "    random.shuffle(index)\n",
        "    data[i][0],data[i][1] = shuffle_list_data(data[i][0],data[i][1])\n",
        "  return data\n",
        "\n",
        "#To further make the data unbalanced by shuffling the mapped array\n",
        "def shuffle_list_data(x, y):\n",
        "  '''\n",
        "  This function is a helper function, shuffles an\n",
        "  array while maintaining the mapping between x and y\n",
        "  '''\n",
        "  inds = list(range(len(x)))\n",
        "  random.shuffle(inds)\n",
        "  return x[inds],y[inds]\n",
        "\n",
        "#The below code snippet converts the split into a data loader(image augmentation is done is this part) \n",
        "#for giving this as an input to the model for training.\n",
        "class CustomImageDataset(Dataset):\n",
        "  '''\n",
        "  A custom Dataset class for images\n",
        "  inputs : numpy array [n_data x shape]\n",
        "  labels : numpy array [n_data (x 1)]\n",
        "  '''\n",
        "  def __init__(self, inputs, labels, transforms=None):\n",
        "      assert inputs.shape[0] == labels.shape[0]\n",
        "      self.inputs = torch.Tensor(inputs)\n",
        "      self.labels = torch.Tensor(labels).long()\n",
        "      self.transforms = transforms \n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      img, label = self.inputs[index], self.labels[index]\n",
        "\n",
        "      if self.transforms is not None:\n",
        "        img = self.transforms(img)\n",
        "\n",
        "      return (img, label)\n",
        "\n",
        "  def __len__(self):\n",
        "      return self.inputs.shape[0]\n",
        "          \n",
        "\n",
        "def get_default_data_transforms(train=True, verbose=True):\n",
        "  transforms_train = {\n",
        "  'cifar10' : transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]),#(0.24703223, 0.24348513, 0.26158784)\n",
        "  }\n",
        "  transforms_eval = {    \n",
        "  'cifar10' : transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "  }\n",
        "  if verbose:\n",
        "    print(\"\\nData preprocessing: \")\n",
        "    for transformation in transforms_train['cifar10'].transforms:\n",
        "      print(' -', transformation)\n",
        "    print()\n",
        "\n",
        "  return (transforms_train['cifar10'], transforms_eval['cifar10'])\n",
        "\n",
        "#The get_data_loader function uses the above helper functions and converts the CIFAR10 dataset into non-IID type\n",
        "\n",
        "def get_data_loaders(nclients,batch_size,classes_pc=10 ,verbose=True ):\n",
        "  \n",
        "  x_train, y_train, x_test, y_test = get_cifar10()\n",
        "\n",
        "  if verbose:\n",
        "    print_image_data_stats(x_train, y_train, x_test, y_test)\n",
        "\n",
        "  transforms_train, transforms_eval = get_default_data_transforms(verbose=False)\n",
        "  \n",
        "  split = split_image_data(x_train, y_train, n_clients=nclients, \n",
        "        classes_per_client=classes_pc, verbose=verbose)\n",
        "  \n",
        "  split_tmp = shuffle_list(split)\n",
        "  \n",
        "  client_loaders = [torch.utils.data.DataLoader(CustomImageDataset(x, y, transforms_train), \n",
        "                                                                batch_size=batch_size, shuffle=True) for x, y in split_tmp]\n",
        "\n",
        "  test_loader  = torch.utils.data.DataLoader(CustomImageDataset(x_test, y_test, transforms_eval), batch_size=100, shuffle=False) \n",
        "\n",
        "  return client_loaders, test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16Ui2sw2m8fY"
      },
      "source": [
        "# Building the Neural Network Model\n",
        "**VGG:** It was proposed by the Visual Geometry Group of Oxford University in 2014 and obtained accurate classification performance on the ImageNet dataset.\n",
        "\n",
        "**VGG19:** 16 convolution layers, 3 Fully Connected layers, 5 MaxPool layers (Summarizing the output of Convolution Layer), and 1 SoftMax layer (Softmax is implemented through a neural network layer just before the output layer. The Softmax layer must have the same number of nodes as the output layer)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLdhxYgo41GG"
      },
      "source": [
        "#################################\n",
        "##### Neural Network model #####\n",
        "#################################\n",
        "\n",
        "#VGG is a deep CNN used to classify images.\n",
        "\n",
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        output = F.log_softmax(out, dim=1)\n",
        "        return output\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wBa6VpnnC3q"
      },
      "source": [
        "# Helper functions for Federated Learning\n",
        "The **baseline_data** function creates a loader for the baseline data on which the clientâ€™s model is retrained before the aggregation of weights on the global server. \n",
        "\n",
        "**â€˜numâ€™** is the number of images on which the retraining of clientâ€™s model on the global server is supposed to take place. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0ivjWEh45lv"
      },
      "source": [
        "def baseline_data(num):\n",
        "  '''\n",
        "  Returns baseline data loader to be used on retraining on global server\n",
        "  Input:\n",
        "        num : size of baseline data\n",
        "  Output:\n",
        "        loader: baseline data loader\n",
        "  '''\n",
        "  xtrain, ytrain, xtmp,ytmp = get_cifar10()\n",
        "  x , y = shuffle_list_data(xtrain, ytrain)\n",
        "\n",
        "  x, y = x[:num], y[:num]\n",
        "  transform, _ = get_default_data_transforms(train=True, verbose=False)\n",
        "  loader = torch.utils.data.DataLoader(CustomImageDataset(x, y, transform), batch_size=16, shuffle=True)\n",
        "\n",
        "  return loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MHx1zosxaWs"
      },
      "source": [
        "The **client_update** function trains the client model on the given private client data. This is the local training round that takes place for every selected client, i.e. num_selected (6 in our case).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "envtYjTl45om"
      },
      "source": [
        "def client_update(client_model, optimizer, train_loader, epoch=5):\n",
        "    \"\"\"\n",
        "    This function updates/trains client model on client data\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    for e in range(epoch):\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            output = client_model(data)\n",
        "            loss = F.nll_loss(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    return loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDNXsMyUxmQG"
      },
      "source": [
        "The **client_sync** function synchronizes the client model (before training) with global weights. It helps in the case when a particular client has not participated in the previous communication rounds, so it makes sure that all the selected clients have the previously trained weights from the global model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wtzxnqv5Ebj"
      },
      "source": [
        "def client_syn(client_model, global_model):\n",
        "  '''\n",
        "  This function synchronizes the client model with global model\n",
        "  '''\n",
        "  client_model.load_state_dict(global_model.state_dict())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0drOkrxxsEU"
      },
      "source": [
        "The **server_aggregate** function aggregates the model weights received from every client and updates the global model with updated weights. Here, the weighted mean of the weights is calculated. In IID part of this code, instead of the weighted mean, the mean is used as an aggregation method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTnISVhm7Klt"
      },
      "source": [
        "def server_aggregate(global_model, client_models,client_lens):\n",
        "    \"\"\"\n",
        "    This function has aggregation method 'wmean'\n",
        "    wmean takes the weighted mean of the weights of models\n",
        "    \"\"\"\n",
        "    total = sum(client_lens)\n",
        "    n = len(client_models)\n",
        "    global_dict = global_model.state_dict()\n",
        "    for k in global_dict.keys():\n",
        "        global_dict[k] = torch.stack([client_models[i].state_dict()[k].float()*(n*client_lens[i]/total) for i in range(len(client_models))], 0).mean(0)\n",
        "    global_model.load_state_dict(global_dict)\n",
        "    for model in client_models:\n",
        "        model.load_state_dict(global_model.state_dict())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XEqUzz3ySsA"
      },
      "source": [
        "The **test** function is the standard function for evaluating the global model with the test dataset. It returns the test loss and test accuracy, which is used for a comparative study of different approaches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXHb8VCP7ReI"
      },
      "source": [
        "def test(global_model, test_loader):\n",
        "    \"\"\"\n",
        "    This function test the global model on test \n",
        "    data and returns test loss and test accuracy \n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            output = global_model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    acc = correct / len(test_loader.dataset)\n",
        "\n",
        "    return test_loss, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCyuQ-7hygsU"
      },
      "source": [
        "# Training the Model\n",
        "Global model, clientâ€™s models are initialized with the VGG19, and training is done on a GPU. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "58150d0349f94f8e86d1e1013be6378c",
            "9e1e60c52c054612babd18aaa7ca57d3",
            "bafaea4c6ed04f90a5ca8677b9ad85f9",
            "b5884797e13c451b9b300608771d87bd",
            "0d1da8df8cd342f8ba17fbee058a286c",
            "31d3eaebab2e4cebbcc9ae30c4df8d6d",
            "afcd1a22f1824fc49d0758d857dcff7e",
            "4f081f12d75041448dd84a61c991118a"
          ]
        },
        "id": "LEl3rzHV7V0J",
        "outputId": "05bb2715-709f-4193-a364-9ea4d41e03c8"
      },
      "source": [
        "############################################\n",
        "#### Initializing models and optimizer  ####\n",
        "############################################\n",
        "\n",
        "#### global model ##########\n",
        "global_model =  VGG('VGG19').cuda()\n",
        "\n",
        "############# client models ###############################\n",
        "#Initialize VGG19 on GPU\n",
        "client_models = [ VGG('VGG19').cuda() for _ in range(num_selected)]\n",
        "for model in client_models:\n",
        "    model.load_state_dict(global_model.state_dict()) ### initial synchronizing with global modle \n",
        "\n",
        "###### optimizers ################\n",
        "#the optimizer (SGD) is defined along with the learning rate.\n",
        "opt = [optim.SGD(model.parameters(), lr=0.1) for model in client_models]\n",
        "\n",
        "####### baseline data ############\n",
        "#the baseline data is added to a loader with â€˜baseline_numâ€™ images, i.e. 100 images as defined previously\n",
        "loader_fixed = baseline_data(baseline_num)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58150d0349f94f8e86d1e1013be6378c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM9mm60A0LW2"
      },
      "source": [
        "The non-IID data is loaded into a train_loader using the above functions, which ensures the data is non-IID. Classes_pc=2, num_clients=20, batch_size=32."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsvgIqRE7ZJK",
        "outputId": "0cfe2761-eb06-440b-ac0f-9f272478e987"
      },
      "source": [
        "###### Loading the data using the above function ######\n",
        "train_loader, test_loader = get_data_loaders(classes_pc=classes_pc, nclients= num_clients,\n",
        "                                                      batch_size=batch_size,verbose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Data: \n",
            " - Train Set: ((50000, 3, 32, 32),(50000,)), Range: [0.000, 255.000], Labels: 0,..,9\n",
            " - Test Set: ((10000, 3, 32, 32),(10000,)), Range: [0.000, 255.000], Labels: 0,..,9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:123: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMPxX9-p7b6i",
        "outputId": "2cb6baac-5d87-4066-ce06-fd4b738eaa4d"
      },
      "source": [
        "#create a list for keeping track of the loss and accuracy of the model on train and test dataset.\n",
        "losses_train = []\n",
        "losses_test = []\n",
        "acc_test = []\n",
        "losses_retrain=[]\n",
        "\n",
        "# Runnining FL\n",
        "'''starts the training of individual clients in communication rounds (num_rounds).\n",
        "In every communication round, first, the selected clients are updated with the global weights. \n",
        "Then the local model is trained on the clientâ€™s device itself, following which the retraining round takes place on the global server. \n",
        "After retraining the clientâ€™s model, the aggregation of weights takes place.'''\n",
        "\n",
        "for r in range(num_rounds):    #Communication round\n",
        "    # select random clients\n",
        "\n",
        "    #selects the num_selected clients from num_clients, i.e. six clients are randomly selected from a total of 20 clients. \n",
        "    #Training at the clientâ€™s device is done \n",
        "    client_idx = np.random.permutation(num_clients)[:num_selected]\n",
        "    client_lens = [len(train_loader[idx]) for idx in client_idx]\n",
        "\n",
        "    # client update\n",
        "    loss = 0\n",
        "    for i in tqdm(range(num_selected)):\n",
        "      client_syn(client_models[i], global_model) #using the client_sync\n",
        "      #where the local models are updated with the global weights before the training, and then client_update function\n",
        "      #is used to start the training\n",
        "      loss += client_update(client_models[i], opt[i], train_loader[client_idx[i]], epochs)\n",
        "    losses_train.append(loss)\n",
        "\n",
        "    ''' Once the local models are trained on the device itself, ensuring the privacy of the private data, they are sent to the global server.\n",
        "        First, the retraining of these models with the baseline data is done. It is followed by the aggregation of these local models\n",
        "        (weights) into one global model. After updating the global model, this global model is used to test the training with the \n",
        "        help of the test function defined before.'''\n",
        "\n",
        "    # server aggregate\n",
        "    #### retraining on the global server\n",
        "    loss_retrain =0\n",
        "    for i in tqdm(range(num_selected)):\n",
        "      loss_retrain+= client_update(client_models[i], opt[i], loader_fixed, epoch=retrain_epochs)\n",
        "    losses_retrain.append(loss_retrain)\n",
        "    \n",
        "    ### Aggregating the models\n",
        "    server_aggregate(global_model, client_models,client_lens)\n",
        "    test_loss, acc = test(global_model, test_loader)\n",
        "    losses_test.append(test_loss)\n",
        "    acc_test.append(acc)\n",
        "    print('%d-th round' % r)\n",
        "    print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss_retrain / num_selected, test_loss, acc))\n",
        "\n",
        "#This process continues for num_rounds, i.e. 150 communication rounds in our case. \n",
        "#6 selected clients, each running 5 local epochs and retaining on the global server with 20 epochs on top of the 150 communication rounds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/6 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:09<00:00, 11.52s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.37s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0-th round\n",
            "average train loss 2.34 | test loss 2.32 | test acc: 0.100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:14<00:00, 12.41s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1-th round\n",
            "average train loss 2.07 | test loss 2.31 | test acc: 0.100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:54<00:00,  9.16s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2-th round\n",
            "average train loss 2.52 | test loss 2.32 | test acc: 0.106\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:20<00:00, 13.35s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3-th round\n",
            "average train loss 2.05 | test loss 2.22 | test acc: 0.120\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:11<00:00, 11.95s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4-th round\n",
            "average train loss 2.18 | test loss 2.31 | test acc: 0.091\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:57<00:00,  9.53s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.33s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5-th round\n",
            "average train loss 2.19 | test loss 2.19 | test acc: 0.203\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:03<00:00, 10.66s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.33s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "6-th round\n",
            "average train loss 1.8 | test loss 2.35 | test acc: 0.079\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:16<00:00, 12.81s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "7-th round\n",
            "average train loss 1.95 | test loss 2.19 | test acc: 0.139\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:03<00:00, 10.64s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.33s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "8-th round\n",
            "average train loss 1.94 | test loss 1.98 | test acc: 0.225\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:59<00:00,  9.84s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.33s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "9-th round\n",
            "average train loss 2.18 | test loss 1.99 | test acc: 0.243\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:15<00:00, 12.62s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10-th round\n",
            "average train loss 2.06 | test loss 1.88 | test acc: 0.235\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:04<00:00, 10.67s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11-th round\n",
            "average train loss 2.15 | test loss 1.93 | test acc: 0.264\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:09<00:00, 11.62s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "12-th round\n",
            "average train loss 1.87 | test loss 1.86 | test acc: 0.270\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:04<00:00, 10.73s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13-th round\n",
            "average train loss 1.8 | test loss 2.18 | test acc: 0.150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:09<00:00, 11.63s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "14-th round\n",
            "average train loss 1.75 | test loss 1.89 | test acc: 0.298\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:10<00:00, 11.69s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "15-th round\n",
            "average train loss 1.88 | test loss 1.88 | test acc: 0.263\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:07<00:00, 11.26s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "16-th round\n",
            "average train loss 1.72 | test loss 1.95 | test acc: 0.285\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:41<00:00,  6.88s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "17-th round\n",
            "average train loss 2.38 | test loss 1.74 | test acc: 0.295\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:55<00:00,  9.21s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "18-th round\n",
            "average train loss 1.68 | test loss 1.86 | test acc: 0.287\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:00<00:00, 10.14s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "19-th round\n",
            "average train loss 1.64 | test loss 2.01 | test acc: 0.246\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:02<00:00, 10.34s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.33s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20-th round\n",
            "average train loss 1.66 | test loss 1.64 | test acc: 0.385\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:43<00:00,  7.21s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.33s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21-th round\n",
            "average train loss 1.37 | test loss 1.85 | test acc: 0.327\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:56<00:00,  9.48s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.33s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "22-th round\n",
            "average train loss 2.19 | test loss 1.69 | test acc: 0.368\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:54<00:00,  9.02s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.33s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "23-th round\n",
            "average train loss 1.34 | test loss 1.63 | test acc: 0.421\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:37<00:00,  6.26s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.33s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "24-th round\n",
            "average train loss 1.32 | test loss 1.66 | test acc: 0.396\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:16<00:00, 12.73s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "25-th round\n",
            "average train loss 0.781 | test loss 1.75 | test acc: 0.429\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:08<00:00, 11.50s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.33s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "26-th round\n",
            "average train loss 1.19 | test loss 1.82 | test acc: 0.408\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:58<00:00,  9.81s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.33s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "27-th round\n",
            "average train loss 1.19 | test loss 1.65 | test acc: 0.448\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:01<00:00, 10.27s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.33s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "28-th round\n",
            "average train loss 1.3 | test loss 2.07 | test acc: 0.404\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:48<00:00,  8.00s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "29-th round\n",
            "average train loss 1.42 | test loss 1.83 | test acc: 0.432\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:06<00:00, 11.14s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "30-th round\n",
            "average train loss 1.4 | test loss 1.67 | test acc: 0.481\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:03<00:00, 10.51s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.33s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "31-th round\n",
            "average train loss 1.24 | test loss 1.72 | test acc: 0.479\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:00<00:00, 10.02s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.33s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "32-th round\n",
            "average train loss 0.77 | test loss 1.9 | test acc: 0.467\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:38<00:00,  6.39s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.35s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "33-th round\n",
            "average train loss 0.977 | test loss 1.81 | test acc: 0.475\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:56<00:00,  9.38s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "34-th round\n",
            "average train loss 0.557 | test loss 2.03 | test acc: 0.462\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:08<00:00, 11.43s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "35-th round\n",
            "average train loss 0.723 | test loss 1.86 | test acc: 0.473\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:53<00:00,  8.98s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.33s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "36-th round\n",
            "average train loss 1.35 | test loss 2.11 | test acc: 0.494\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:48<00:00,  8.15s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.33s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "37-th round\n",
            "average train loss 1.03 | test loss 1.72 | test acc: 0.499\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:01<00:00, 10.29s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "38-th round\n",
            "average train loss 1 | test loss 1.79 | test acc: 0.470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:07<00:00, 11.32s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.33s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "39-th round\n",
            "average train loss 1.11 | test loss 1.79 | test acc: 0.536\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:20<00:00, 13.46s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "40-th round\n",
            "average train loss 0.302 | test loss 2.02 | test acc: 0.487\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:06<00:00, 11.08s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "41-th round\n",
            "average train loss 0.748 | test loss 1.86 | test acc: 0.527\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:09<00:00, 11.56s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.33s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "42-th round\n",
            "average train loss 0.773 | test loss 1.96 | test acc: 0.526\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:52<00:00,  8.73s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "43-th round\n",
            "average train loss 0.608 | test loss 1.84 | test acc: 0.555\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:15<00:00, 12.64s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "44-th round\n",
            "average train loss 0.429 | test loss 1.85 | test acc: 0.564\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:53<00:00,  8.90s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "45-th round\n",
            "average train loss 0.762 | test loss 1.72 | test acc: 0.555\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:13<00:00, 12.24s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "46-th round\n",
            "average train loss 0.435 | test loss 1.66 | test acc: 0.572\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:57<00:00,  9.60s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "47-th round\n",
            "average train loss 0.96 | test loss 1.81 | test acc: 0.420\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:00<00:00, 10.02s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "48-th round\n",
            "average train loss 0.392 | test loss 1.81 | test acc: 0.556\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:07<00:00, 11.28s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.34s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "49-th round\n",
            "average train loss 0.458 | test loss 1.77 | test acc: 0.554\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}